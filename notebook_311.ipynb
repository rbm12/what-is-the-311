{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from math import log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the data set from the city of Boston:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "olddf = pd.read_csv('boston_311_service_requests.csv', low_memory=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Making a copy of the dataframe, but saving the old one in case I want to refer to it later:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = olddf.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing cases that were listed as duplicates or opened/submitted in error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[~(df.closure_reason.str.contains('uplicate', na=False))]\n",
    "df = df[~(df.closure_reason.str.contains('ubmitted in error', na=False))]\n",
    "df = df[~(df.closure_reason.str.contains('pened in error', na=False))]\n",
    "df = df[~(df.closure_reason.str.contains('NOBASE', na=False))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to consider only cases that are closed and with a neighborhood listed, so drop rows where those columns are null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(subset=['closed_dt', 'neighborhood'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding a new column that gives how long the case was open:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['time_diff'] = pd.to_datetime(df['closed_dt']) - pd.to_datetime(df['open_dt'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the \"Boston\" neighborhood because it is ambiguous:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.neighborhood != 'Boston']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the \"Chestnut Hill\" neighborhood because there were very few calls form it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.neighborhood != 'Chestnut Hill']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combining similar (in location) neighborhoods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.replace({'neighborhood': {'Allston': 'Allston / Brighton', \\\n",
    "                             'Brighton': 'Allston / Brighton', \\\n",
    "                             'Greater Mattapan': 'Mattapan', \\\n",
    "                             'South Boston / South Boston Waterfront':'South Boston'}})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Removing the features that I don't use. Some of these features are geographic, time, too related to what I want to predict, or not something a Boston resident may easily know. Removing closure_reason, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['pwd_district', \\\n",
    "                        'police_district', 'ward','precinct','location_zipcode',\\\n",
    "                        'subject', 'case_enquiry_id', \\\n",
    "                        'fire_district', 'city_council_district',\\\n",
    "                        'neighborhood_services_district', \\\n",
    "                         'ontime', 'case_status', \\\n",
    "                        'case_title', 'queue', 'department','closedphoto',\\\n",
    "                        'location','location_street_name', \\\n",
    "                        'latitude', 'longitude', 'target_dt',\\\n",
    "                        'open_dt', 'closed_dt', 'closure_reason'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If a photo was submitted (has a URL) turning it into a 1. If it was not submitted, turning the null into a 0:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.submittedphoto = ~df.submittedphoto.isnull()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the time_diff (which is a time delta) into the number of seconds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec = lambda t: t.total_seconds() \n",
    "df.time_diff = df.time_diff.map(sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some of the cases were listed as open for a very short time or even for negative time, so removing cases open for less than two minutes:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df.time_diff > 120]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Converting the time difference to days since that is more on the scale that I am interested in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.time_diff = df.time_diff/(60*60*24)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Saving a version of the dataframe in case I want to look at it without the dummy variables I am about to introduce. It also keeps the reason column:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleandf = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping the reason column since each type only corresponds with one reason:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns = ['reason'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turning the categorical variables (neighborhood, type, source) into dummy variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I noted that dropping the extra dummy did not improve the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time_diff was highly skewed, so I will perform a log transform.To to this, I need to map with a log function. I also want to cap the maximum value at 7 (meaning if log(time_diff) > 7, then it will give 7 instead). Note that exp(7) is approximately 1096 days, which is about 3 years. Creating the log function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def customlog(x): #input days, want output to be between -infinity and 7\n",
    "    return min(log(x), 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting into training and test sets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.drop(columns = ['time_diff']), df.time_diff, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Want to train the model on the time_diff after the log transform. Won't need to do that to the test set since I'm interested in how it works as a predictor for the actual days. Transforming y_train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_y_train = y_train.map(customlog)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I ultimately used a random forest regression model. Defining that here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forestmodel(Xtrain, ytrain):\n",
    "    fregr = RandomForestRegressor()\n",
    "    fregr.fit(Xtrain, ytrain)\n",
    "    return fregr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting the model to my training data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rebeccamoran/anaconda3/lib/python3.7/site-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "F = forestmodel(X_train, log_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some of the models, I was getting very large predictions, so I wanted to cap them at exp(7) (this is why I defined the log the way I did above). Capping the output at 7:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_y_pred = np.minimum(F.predict(X_test), 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exponentiating the output so that it is in days, not log(days):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_y_pred = np.exp(f_y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the mean absolute error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAE = ((abs(f_y_pred - y_test)).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparing the MAE to the mean difference between the mean and the test values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4257967222589367\n"
     ]
    }
   ],
   "source": [
    "print(1 - MAE/((abs(y_train.mean() - y_test)).mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, my predictions are 42.6% better than predicting the mean time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
